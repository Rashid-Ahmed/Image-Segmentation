{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Image Segmentation pipeline"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "For the purpose of this task we will be using PASCAL VOC datset. The dataset contains a total of 2913 images with segmentation annotations. Code in the cell below will download the code and extract the dataset."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!wget http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar\r\n",
    "!tar -xvf VOCtrainval_11-May-2012.tar"
   ],
   "outputs": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 216508,
     "status": "ok",
     "timestamp": 1610647557551,
     "user": {
      "displayName": "dikshant gupta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjGExYbOwwSrmoe8LVKyI5XQQl2f1k4HbD2Vx5Vpg=s64",
      "userId": "01845807612441668603"
     },
     "user_tz": -60
    },
    "id": "zM_t4c-S3k31",
    "outputId": "b960fc6f-f665-42c4-db32-1b6ced9dfb60"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!pip install scipy==1.1.0"
   ],
   "outputs": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 255947,
     "status": "ok",
     "timestamp": 1610647598272,
     "user": {
      "displayName": "dikshant gupta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjGExYbOwwSrmoe8LVKyI5XQQl2f1k4HbD2Vx5Vpg=s64",
      "userId": "01845807612441668603"
     },
     "user_tz": -60
    },
    "id": "6lvs9XIpBaI0",
    "outputId": "40b9ac25-7e3b-47ae-b396-dc3dcb21b608"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.1 Loading the dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import os\r\n",
    "from os.path import join as pjoin\r\n",
    "import collections\r\n",
    "import json\r\n",
    "import torch\r\n",
    "import imageio\r\n",
    "import numpy as np\r\n",
    "import scipy.misc as m\r\n",
    "import scipy.io as io\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import glob\r\n",
    "\r\n",
    "from PIL import Image\r\n",
    "from tqdm import tqdm\r\n",
    "from torch.utils import data\r\n",
    "from torchvision import transforms\r\n",
    "\r\n",
    "\r\n",
    "class pascalVOCDataset(data.Dataset):\r\n",
    "    \"\"\"Data loader for the Pascal VOC semantic segmentation dataset.\r\n",
    "\r\n",
    "    Annotations from both the original VOC data (which consist of RGB images\r\n",
    "    in which colours map to specific classes) and the SBD (Berkely) dataset\r\n",
    "    (where annotations are stored as .mat files) are converted into a common\r\n",
    "    `label_mask` format.  Under this format, each mask is an (M,N) array of\r\n",
    "    integer values from 0 to 21, where 0 represents the background class.\r\n",
    "\r\n",
    "    The label masks are stored in a new folder, called `pre_encoded`, which\r\n",
    "    is added as a subdirectory of the `SegmentationClass` folder in the\r\n",
    "    original Pascal VOC data layout.\r\n",
    "\r\n",
    "    A total of five data splits are provided for working with the VOC data:\r\n",
    "        train: The original VOC 2012 training data - 1464 images\r\n",
    "        val: The original VOC 2012 validation data - 1449 images\r\n",
    "        trainval: The combination of `train` and `val` - 2913 images\r\n",
    "        train_aug: The unique images present in both the train split and\r\n",
    "                   training images from SBD: - 8829 images (the unique members\r\n",
    "                   of the result of combining lists of length 1464 and 8498)\r\n",
    "        train_aug_val: The original VOC 2012 validation data minus the images\r\n",
    "                   present in `train_aug` (This is done with the same logic as\r\n",
    "                   the validation set used in FCN PAMI paper, but with VOC 2012\r\n",
    "                   rather than VOC 2011) - 904 images\r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "    def __init__(\r\n",
    "        self,\r\n",
    "        root,\r\n",
    "        sbd_path=None,\r\n",
    "        split=\"train_aug\",\r\n",
    "        is_transform=False,\r\n",
    "        img_size=512,\r\n",
    "        augmentations=None,\r\n",
    "        img_norm=True,\r\n",
    "        test_mode=False,\r\n",
    "    ):\r\n",
    "        self.root = root\r\n",
    "        self.sbd_path = sbd_path\r\n",
    "        self.split = split\r\n",
    "        self.is_transform = is_transform\r\n",
    "        self.augmentations = augmentations\r\n",
    "        self.img_norm = img_norm\r\n",
    "        self.test_mode = test_mode\r\n",
    "        self.n_classes = 21\r\n",
    "        self.mean = np.array([104.00699, 116.66877, 122.67892])\r\n",
    "        self.files = collections.defaultdict(list)\r\n",
    "        self.img_size = img_size if isinstance(img_size, tuple) else (img_size, img_size)\r\n",
    "\r\n",
    "        if not self.test_mode:\r\n",
    "            for split in [\"train\", \"val\", \"trainval\"]:\r\n",
    "                path = pjoin(self.root, \"ImageSets/Segmentation\", split + \".txt\")\r\n",
    "                file_list = tuple(open(path, \"r\"))\r\n",
    "                file_list = [id_.rstrip() for id_ in file_list]\r\n",
    "                self.files[split] = file_list\r\n",
    "            self.setup_annotations()\r\n",
    "\r\n",
    "        self.tf = transforms.Compose(\r\n",
    "            [\r\n",
    "                # add more trasnformations as you see fit\r\n",
    "                transforms.ToTensor(),\r\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\r\n",
    "            ]\r\n",
    "        )\r\n",
    "\r\n",
    "    def __len__(self):\r\n",
    "        return len(self.files[self.split])\r\n",
    "\r\n",
    "    def __getitem__(self, index):\r\n",
    "        im_name = self.files[self.split][index]\r\n",
    "        im_path = pjoin(self.root, \"JPEGImages\", im_name + \".jpg\")\r\n",
    "        lbl_path = pjoin(self.root, \"SegmentationClass/pre_encoded\", im_name + \".png\")\r\n",
    "        im = Image.open(im_path)\r\n",
    "        lbl = Image.open(lbl_path)\r\n",
    "        if self.augmentations is not None:\r\n",
    "            im, lbl = self.augmentations(im, lbl)\r\n",
    "        if self.is_transform:\r\n",
    "            im, lbl = self.transform(im, lbl)\r\n",
    "        return im, torch.clamp(lbl, max=20)\r\n",
    "\r\n",
    "    def transform(self, img, lbl):\r\n",
    "        if self.img_size == (\"same\", \"same\"):\r\n",
    "            pass\r\n",
    "        else:\r\n",
    "            img = img.resize((self.img_size[0], self.img_size[1]))  # uint8 with RGB mode\r\n",
    "            lbl = lbl.resize((self.img_size[0], self.img_size[1]))\r\n",
    "        img = self.tf(img)\r\n",
    "        lbl = torch.from_numpy(np.array(lbl)).long()\r\n",
    "        lbl[lbl == 255] = 0\r\n",
    "        return img, lbl\r\n",
    "\r\n",
    "    def get_pascal_labels(self):\r\n",
    "        \"\"\"Load the mapping that associates pascal classes with label colors\r\n",
    "\r\n",
    "        Returns:\r\n",
    "            np.ndarray with dimensions (21, 3)\r\n",
    "        \"\"\"\r\n",
    "        return np.asarray(\r\n",
    "            [\r\n",
    "                [0, 0, 0],\r\n",
    "                [128, 0, 0],\r\n",
    "                [0, 128, 0],\r\n",
    "                [128, 128, 0],\r\n",
    "                [0, 0, 128],\r\n",
    "                [128, 0, 128],\r\n",
    "                [0, 128, 128],\r\n",
    "                [128, 128, 128],\r\n",
    "                [64, 0, 0],\r\n",
    "                [192, 0, 0],\r\n",
    "                [64, 128, 0],\r\n",
    "                [192, 128, 0],\r\n",
    "                [64, 0, 128],\r\n",
    "                [192, 0, 128],\r\n",
    "                [64, 128, 128],\r\n",
    "                [192, 128, 128],\r\n",
    "                [0, 64, 0],\r\n",
    "                [128, 64, 0],\r\n",
    "                [0, 192, 0],\r\n",
    "                [128, 192, 0],\r\n",
    "                [0, 64, 128],\r\n",
    "            ]\r\n",
    "        )\r\n",
    "\r\n",
    "    def encode_segmap(self, mask):\r\n",
    "        \"\"\"Encode segmentation label images as pascal classes\r\n",
    "\r\n",
    "        Args:\r\n",
    "            mask (np.ndarray): raw segmentation label image of dimension\r\n",
    "              (M, N, 3), in which the Pascal classes are encoded as colours.\r\n",
    "\r\n",
    "        Returns:\r\n",
    "            (np.ndarray): class map with dimensions (M,N), where the value at\r\n",
    "            a given location is the integer denoting the class index.\r\n",
    "        \"\"\"\r\n",
    "        mask = mask.astype(int)\r\n",
    "        label_mask = np.zeros((mask.shape[0], mask.shape[1]), dtype=np.int16)\r\n",
    "        for ii, label in enumerate(self.get_pascal_labels()):\r\n",
    "            label_mask[np.where(np.all(mask == label, axis=-1))[:2]] = ii\r\n",
    "        label_mask = label_mask.astype(int)\r\n",
    "        # print(np.unique(label_mask))\r\n",
    "        return label_mask\r\n",
    "\r\n",
    "    def decode_segmap(self, label_mask, plot=False):\r\n",
    "        \"\"\"Decode segmentation class labels into a color image\r\n",
    "\r\n",
    "        Args:\r\n",
    "            label_mask (np.ndarray): an (M,N) array of integer values denoting\r\n",
    "              the class label at each spatial location.\r\n",
    "            plot (bool, optional): whether to show the resulting color image\r\n",
    "              in a figure.\r\n",
    "\r\n",
    "        Returns:\r\n",
    "            (np.ndarray, optional): the resulting decoded color image.\r\n",
    "        \"\"\"\r\n",
    "        label_colours = self.get_pascal_labels()\r\n",
    "        r = label_mask.copy()\r\n",
    "        g = label_mask.copy()\r\n",
    "        b = label_mask.copy()\r\n",
    "        for ll in range(0, self.n_classes):\r\n",
    "            r[label_mask == ll] = label_colours[ll, 0]\r\n",
    "            g[label_mask == ll] = label_colours[ll, 1]\r\n",
    "            b[label_mask == ll] = label_colours[ll, 2]\r\n",
    "        rgb = np.zeros((label_mask.shape[0], label_mask.shape[1], 3))\r\n",
    "        rgb[:, :, 0] = r / 255.0\r\n",
    "        rgb[:, :, 1] = g / 255.0\r\n",
    "        rgb[:, :, 2] = b / 255.0\r\n",
    "        if plot:\r\n",
    "            plt.imshow(rgb)\r\n",
    "            plt.show()\r\n",
    "        else:\r\n",
    "            return rgb\r\n",
    "\r\n",
    "    def setup_annotations(self):\r\n",
    "        \"\"\"Sets up Berkley annotations by adding image indices to the\r\n",
    "        `train_aug` split and pre-encode all segmentation labels into the\r\n",
    "        common label_mask format (if this has not already been done). This\r\n",
    "        function also defines the `train_aug` and `train_aug_val` data splits\r\n",
    "        according to the description in the class docstring\r\n",
    "        \"\"\"\r\n",
    "        sbd_path = self.sbd_path\r\n",
    "        target_path = pjoin(self.root, \"SegmentationClass/pre_encoded\")\r\n",
    "        if not os.path.exists(target_path):\r\n",
    "            os.makedirs(target_path)\r\n",
    "        train_aug = self.files[\"train\"]\r\n",
    "\r\n",
    "        # keep unique elements (stable)\r\n",
    "        train_aug = [train_aug[i] for i in sorted(np.unique(train_aug, return_index=True)[1])]\r\n",
    "        self.files[\"train_aug\"] = train_aug\r\n",
    "        set_diff = set(self.files[\"val\"]) - set(train_aug)  # remove overlap\r\n",
    "        self.files[\"train_aug_val\"] = list(set_diff)\r\n",
    "\r\n",
    "        pre_encoded = glob.glob(pjoin(target_path, \"*.png\"))\r\n",
    "        expected = np.unique(self.files[\"train_aug\"] + self.files[\"val\"]).size\r\n",
    "\r\n",
    "        if len(pre_encoded) != expected:\r\n",
    "            print(\"Pre-encoding segmentation masks...\")\r\n",
    "\r\n",
    "            for ii in tqdm(self.files[\"trainval\"]):\r\n",
    "                fname = ii + \".png\"\r\n",
    "                lbl_path = pjoin(self.root, \"SegmentationClass\", fname)\r\n",
    "                lbl = self.encode_segmap(m.imread(lbl_path))\r\n",
    "                lbl = m.toimage(lbl, high=lbl.max(), low=lbl.min())\r\n",
    "                m.imsave(pjoin(target_path, fname), lbl)\r\n",
    "\r\n",
    "        assert expected == 2913, \"unexpected dataset sizes\""
   ],
   "outputs": [],
   "metadata": {
    "executionInfo": {
     "elapsed": 259698,
     "status": "ok",
     "timestamp": 1610647602623,
     "user": {
      "displayName": "dikshant gupta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjGExYbOwwSrmoe8LVKyI5XQQl2f1k4HbD2Vx5Vpg=s64",
      "userId": "01845807612441668603"
     },
     "user_tz": -60
    },
    "id": "qunDv45j24Mg"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.2 Define the model architecture\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import torch.nn as nn\r\n",
    "import torch.nn.functional as F\r\n",
    "\r\n",
    "class Segnet(nn.Module):\r\n",
    "  \r\n",
    "    def __init__(self):\r\n",
    "        super(Segnet, self).__init__()\r\n",
    "        #define the layers for your model\r\n",
    "        self.conv1 = nn.Conv2d(3, 64, 3, padding = 1)\r\n",
    "        self.norm1 = nn.BatchNorm2d(64)\r\n",
    "        self.conv2 = nn.Conv2d(64, 64, 3, padding = 1)\r\n",
    "        self.norm2 = nn.BatchNorm2d(64)\r\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\r\n",
    "\r\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding = 1)\r\n",
    "        self.norm3 = nn.BatchNorm2d(128)\r\n",
    "        self.conv4 = nn.Conv2d(128, 256, 3, padding = 1)\r\n",
    "        self.norm4 = nn.BatchNorm2d(256)\r\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\r\n",
    "            \r\n",
    "        self.conv5 = nn.Conv2d(256, 512, 3, padding = 1)\r\n",
    "        self.norm5 = nn.BatchNorm2d(512)\r\n",
    "        \r\n",
    "        self.upsample1 = nn.Upsample(scale_factor = 2, mode='bilinear')\r\n",
    "        self.conv6 = nn.Conv2d(512, 256, 3, padding = 1)\r\n",
    "        self.norm6 = nn.BatchNorm2d(256)\r\n",
    "        \r\n",
    "        self.upsample2 = nn.Upsample(scale_factor = 2, mode='bilinear')\r\n",
    "        self.conv7 = nn.Conv2d(256, 128, 3, padding = 1)\r\n",
    "        self.norm7 = nn.BatchNorm2d(128)\r\n",
    "        self.conv8 = nn.Conv2d(128, 64, 3, padding = 1)\r\n",
    "        self.norm8 = nn.BatchNorm2d(64)\r\n",
    "        \r\n",
    "        self.conv9 = nn.Conv2d(64, 21, 3, padding = 1)\r\n",
    "        \r\n",
    "    def forward(self, x):\r\n",
    "    #define the forward pass\r\n",
    "        x = F.relu(self.conv1(x))\r\n",
    "        x = self.norm1(x)\r\n",
    "        x = F.relu(self.conv2(x))\r\n",
    "        x = self.norm2(x)\r\n",
    "        x = self.pool1(x)\r\n",
    "        \r\n",
    "        x = F.relu(self.conv3(x))\r\n",
    "        x = self.norm3(x)\r\n",
    "        x = F.relu(self.conv4(x))\r\n",
    "        x = self.norm4(x)\r\n",
    "        x = self.pool2(x)\r\n",
    "        \r\n",
    "        \r\n",
    "        x = F.relu(self.conv5(x))\r\n",
    "        x = self.norm5(x)\r\n",
    "        x = self.upsample1(x)\r\n",
    "        \r\n",
    "        x = F.relu(self.conv6(x))\r\n",
    "        x = self.norm6(x)\r\n",
    "        x = self.upsample2(x)\r\n",
    "        \r\n",
    "        x = F.relu(self.conv7(x))\r\n",
    "        x = self.norm7(x)\r\n",
    "        x = F.relu(self.conv8(x))\r\n",
    "        x = self.norm8(x)\r\n",
    "        x = F.softmax(self.conv9(x))\r\n",
    "        \r\n",
    "        return x\r\n",
    "    "
   ],
   "outputs": [],
   "metadata": {
    "executionInfo": {
     "elapsed": 258581,
     "status": "ok",
     "timestamp": 1610647602625,
     "user": {
      "displayName": "dikshant gupta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjGExYbOwwSrmoe8LVKyI5XQQl2f1k4HbD2Vx5Vpg=s64",
      "userId": "01845807612441668603"
     },
     "user_tz": -60
    },
    "id": "CatAsvH3GTXs"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Creating an instance of the model defined above. \r\n",
    "# You can modify it incase you need to pass paratemers to the constructor.\r\n",
    "model = Segnet()"
   ],
   "outputs": [],
   "metadata": {
    "executionInfo": {
     "elapsed": 11035,
     "status": "ok",
     "timestamp": 1610647664665,
     "user": {
      "displayName": "dikshant gupta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjGExYbOwwSrmoe8LVKyI5XQQl2f1k4HbD2Vx5Vpg=s64",
      "userId": "01845807612441668603"
     },
     "user_tz": -60
    },
    "id": "QfQiOnEkGZat"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.3 Hyperparameters"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "local_path = 'VOCdevkit/VOC2012/' # modify it according to your device\r\n",
    "bs = 32\r\n",
    "epochs = 10"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.4 Dataset and Dataloader"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# dataloader variable\r\n",
    "trainloader = pascalVOCDataset(local_path)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.5 Loss fuction and Optimizer"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#because of uneven classes\r\n",
    "samples = np.array([203289120, 335520, 267840, 227520, 233280, 197280, 132480, 154080, 177120, 259200, 178560, 192960,\r\n",
    "          485280, 385920, 1087200, 168334560, 1333440, 216000, 500000, 50000000, 50000000])\r\n",
    "samples = 1 - samples/np.sum(samples)\r\n",
    "class_weights = torch.FloatTensor(samples)\r\n",
    "\r\n",
    "\r\n",
    "# loss function\r\n",
    "loss_f = nn.CrossEntropyLoss(weight = class_weights)\r\n",
    "\r\n",
    "# optimizer variable\r\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.03)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.6 Training the model\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for _ in range(epochs):\r\n",
    "    for i, d in enumerate(trainloader):\r\n",
    "        if i%32 == 0:\r\n",
    "            # new batch \r\n",
    "            images_batch, labels_batch = d\r\n",
    "            images_batch = torch.reshape(images_batch, (1,3,512,512))\r\n",
    "            labels_batch = torch.reshape(labels_batch, (1,512,512))\r\n",
    "        else:\r\n",
    "            images_batch = torch.cat((images_batch, torch.reshape(d[0], (1,3,512,512))), 0)\r\n",
    "            labels_batch = torch.cat((labels_batch, torch.reshape(d[1], (1,512,512))), 0)\r\n",
    "            #after every batch of 32 train\r\n",
    "            if (i+1)%32==0:\r\n",
    "                optimizer.zero_grad()\r\n",
    "                outputs = model(images_batch)\r\n",
    "                loss = loss_f(outputs, labels_batch)\r\n",
    "                loss.backward()\r\n",
    "                optimizer.step()\r\n",
    "    #save model\r\n",
    "    torch.save(model.state_dict(), \"/home/rani00001/checkpoints/epoch\"+str(_)+\".pt\")"
   ],
   "outputs": [],
   "metadata": {
    "id": "Xz08hSdPKODm"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.7 Evaluate your model\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def evaluate(ground_truth, predictions):\r\n",
    "    \r\n",
    "    #flatten arrays to 1D\r\n",
    "    ground_truth = torch.flatten(ground_truth).numpy()\r\n",
    "    predictions = torch.flatten(predictions).numpy()\r\n",
    "    \r\n",
    "    if np.unique(ground_truth).shape != np.unique(predictions).shape:\r\n",
    "        for i in range(18):\r\n",
    "            predictions[i] = i\r\n",
    "    predictions[predictions == 18] = 0\r\n",
    "    predictions[predictions == 19] = 0\r\n",
    "    predictions[predictions == 20] = 0\r\n",
    "    \r\n",
    "    # calculate metrics\r\n",
    "    f1_score = metrics.f1_score(ground_truth, predictions, average = 'micro')\r\n",
    "    #make onehot of these arrays for auc score\r\n",
    "    onehot_predictions = np.array(pd.get_dummies(predictions))\r\n",
    "    onehot_truth = np.array(pd.get_dummies(ground_truth))\r\n",
    "    \r\n",
    "    auc_score = metrics.roc_auc_score(onehot_truth, onehot_predictions, multi_class = 'ovr')\r\n",
    "    dice_coefficient = dice(ground_truth, predictions) \r\n",
    "        \r\n",
    "    return f1_score, auc_score, dice_coefficient\r\n",
    "\r\n",
    "def dice(ground_truth, predictions):\r\n",
    "    #calculating dice coefficient \r\n",
    "    \r\n",
    "    print (np.sum(ground_truth == predictions)/ground_truth.shape[0])\r\n",
    "    classes = 21\r\n",
    "    for i in range(classes):\r\n",
    "        #calculate TP, FP and FN for each class\r\n",
    "        TP = np.sum(ground_truth[ground_truth == predictions] == i)\r\n",
    "        wrongs = ground_truth != predictions\r\n",
    "        FN = np.sum(ground_truth[wrongs] == i)\r\n",
    "        FP = np.sum(predictions[wrongs] == i)\r\n",
    "        if (TP + FP + FN ==0):\r\n",
    "            dice_coefficient = 0\r\n",
    "        else:\r\n",
    "            dice_coefficient = (2*TP)/((2*TP) + FP + FN)\r\n",
    "        \r\n",
    "        if i ==0:\r\n",
    "            dice_coefficients = np.array(dice_coefficient)\r\n",
    "        else:\r\n",
    "            dice_coefficients = np.append(dice_coefficients, dice_coefficient)\r\n",
    "    return (np.sum(dice_coefficients))/21"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.8 Plot the evaluation metrics against epochs\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "\r\n",
    "for epoch in range(epochs):\r\n",
    "    model_load = Segnet()\r\n",
    "    model_load.load_state_dict(torch.load(\"/home/rani00001/checkpoints/epoch\"+str(epoch)+\".pt\"))\r\n",
    "    with torch.no_grad():\r\n",
    "        for i, data in enumerate(trainloader):\r\n",
    "            # predict in batches of 32\r\n",
    "            if i%32 == 0:\r\n",
    "                images_batch, labels_batch = d\r\n",
    "                images_batch = torch.reshape(images_batch, (1,3,512,512))\r\n",
    "                labels_batch = torch.reshape(labels_batch, (1,512,512))\r\n",
    "            else:\r\n",
    "                images_batch = torch.cat((images_batch, torch.reshape(d[0], (1,3,512,512))), 0)\r\n",
    "                labels_batch = torch.cat((labels_batch, torch.reshape(d[1], (1,512,512))), 0)          \r\n",
    "                if (i+1)%32==0:\r\n",
    "                    outputs = model_load(images_batch)\r\n",
    "                    _, predicted = torch.max(outputs, 1)\r\n",
    "                    if i == 31:\r\n",
    "                        ground_truth = labels_batch\r\n",
    "                        predictions = predicted\r\n",
    "                    else:\r\n",
    "                        ground_truth = torch.cat((ground_truth, labels_batch), 0)\r\n",
    "                        predictions = torch.cat((predictions, predicted), 0)\r\n",
    "                        \r\n",
    "    f1_score, auc_score, dice_coefficient = evaluate(ground_truth, predictions)\r\n",
    "    if epoch == 0:\r\n",
    "        f1_scores = np.array(f1_score)\r\n",
    "        auc_scores = np.array(auc_score)\r\n",
    "        dice_coefficients = np.array(dice_coefficient)\r\n",
    "\r\n",
    "    else:\r\n",
    "        f1_scores = np.append(f1_scores, f1_score)\r\n",
    "        auc_scores = np.append(auc_scores, auc_score)\r\n",
    "        dice_coefficients = np.append(dice_coefficients ,dice_coefficient)       \r\n",
    "        \r\n",
    "epochs_list = np.arange(epochs)\r\n",
    "\r\n",
    "\r\n",
    "plt.plot(epochs_list, f1_scores)\r\n",
    "strFile = \"f1_scores.png\"\r\n",
    "if os.path.isfile(strFile):\r\n",
    "    os.remove(strFile)\r\n",
    "plt.savefig(strFile)\r\n",
    "plt.show()\r\n",
    "plt.close()\r\n",
    "plt.plot(epochs_list, auc_scores)\r\n",
    "strFile = \"auc_scores.png\"\r\n",
    "if os.path.isfile(strFile):\r\n",
    "    os.remove(strFile)\r\n",
    "plt.savefig(strFile)\r\n",
    "plt.show()\r\n",
    "plt.close()\r\n",
    "plt.plot(epochs_list, dice_coeficients)\r\n",
    "strFile = \"dice_coeficients.png\"\r\n",
    "if os.path.isfile(strFile):\r\n",
    "    os.remove(strFile)\r\n",
    "plt.savefig(strFile)\r\n",
    "plt.show()\r\n",
    "plt.close()\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.9 Visualize results"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "colour_mappings = trainloader.get_pascal_labels()\r\n",
    "mean = torch.from_numpy(np.array([0.485, 0.456, 0.406]))\r\n",
    "std = torch.from_numpy(np.array([0.229, 0.224, 0.225]))\r\n",
    "\r\n",
    "\r\n",
    "with torch.no_grad():\r\n",
    "        for i, data in enumerate(trainloader):\r\n",
    "\r\n",
    "            if i == 0:\r\n",
    "                images_batch, labels_batch = data\r\n",
    "                images_batch = torch.reshape(images_batch, (1,3,512,512))\r\n",
    "            else:\r\n",
    "                images_batch = torch.cat((images_batch, torch.reshape(data[0], (1,3,512,512))), 0)\r\n",
    "            if i==9:\r\n",
    "                    outputs = model(images_batch)\r\n",
    "                    _, predictions = torch.max(outputs, 1)\r\n",
    "                    break\r\n",
    "\r\n",
    "segmentation_batch = images_batch\r\n",
    "\r\n",
    "for i in range(segmentation_batch.shape[0]):\r\n",
    "    images_batch[i] = torch.add(torch.mul(images_batch[i], std), mean)\r\n",
    "    image = transforms.ToPILImage()(images_batch[i])\r\n",
    "    \r\n",
    "    strFile = \"image \" + str(i)+\".JPEG\"\r\n",
    "    if os.path.isfile(strFile):\r\n",
    "        os.remove(strFile)\r\n",
    "    image.save(\"image \" + str(i), \"JPEG\")\r\n",
    "    \r\n",
    "    segmentation_batch[i] = images_batch[i]\r\n",
    "    \r\n",
    "    for j in range(512):\r\n",
    "        for k in range(512):\r\n",
    "            Class = predictions[i][j][k]\r\n",
    "            segmentation_batch[i][:, j, k] = torch.from_numpy(np.asarray(colour_mappings[int(Class)]))\r\n",
    "            \r\n",
    "    mask = transforms.ToPILImage()(segmentation_batch[i])\r\n",
    "    \r\n",
    "    strFile = \"segmentation mask \" + str(i)+\".JPEG\"\r\n",
    "    if os.path.isfile(strFile):\r\n",
    "        os.remove(strFile)\r\n",
    "    mask.save(\"segmentation mask \" + str(i), \"JPEG\")"
   ],
   "outputs": [],
   "metadata": {
    "id": "3NS50IL_c7Mf"
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNWH9Es3iVeYFlc2UXzgAC7",
   "collapsed_sections": [],
   "mount_file_id": "1VGd6BoJzZfNjljQpW2Y1nDcze9QND0pA",
   "name": "Copy of segmentation_1.ipynb",
   "provenance": [
    {
     "file_id": "1VGd6BoJzZfNjljQpW2Y1nDcze9QND0pA",
     "timestamp": 1610458540688
    }
   ]
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "interpreter": {
   "hash": "74f384b0bbbb1c08ef72970c1d90ffbcfadaf7927b59b3f5defbe35bb2c0d80e"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}